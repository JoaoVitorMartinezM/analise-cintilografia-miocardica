# --- Imports ---
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, roc_auc_score, 
                           accuracy_score, precision_score, recall_score, f1_score, roc_curve)
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE
import warnings
warnings.filterwarnings('ignore')

# Define color palette
COLORS = {
    'primary': '#980000',    # Deep red
    'secondary': '#ED7D31',  # Orange  
    'tertiary': '#F1C232',   # Yellow
    'quaternary': '#70AD47'  # Green
}

# Feature name translation mapping
FEATURE_TRANSLATION = {
    'IDADE': 'Age',
    'PESO (kg)': 'Weight (kg)',
    'ALTURA (m)': 'Height (m)',
    'IMC': 'BMI',
    'ATIVIDADE (mCi) Repouso': 'Rest Activity (mCi)',
    'ATIVIDADE (mCi) Esfor√ßo': 'Stress Activity (mCi)',
    'N¬∫ REPETI√á√ÉO\nTOTAL': 'Total Repetitions',
    'SEXO_M': 'Gender_Male',
    'ETAPA_1': 'Stage_1',
    'ETAPA_2': 'Stage_2',
    'CAFE√çNA_Sim': 'Caffeine_Yes',
    'CAFE√çNA_True': 'Caffeine_Yes',
    'CAFE√çNA': 'Caffeine'
}

def translate_feature_names(feature_series_or_list):
    """Translate feature names from Portuguese to English"""
    if hasattr(feature_series_or_list, 'index'):
        # It's a pandas Series
        translated_index = []
        for name in feature_series_or_list.index:
            # Skip ID-related features and timing variables
            excluded_features = ['ID PACIENTE', 'Unnamed: 0', 'Patient ID', 'Index',
                               'DELTA Repouso', 'DELTA Esfor√ßo', 'TEMPO TOTAL ATIVIDADE', 
                               'TEMPO PERMANENCIA', 'Rest Delta', 'Stress Delta', 
                               'Total Activity Time', 'Stay Duration']
            if name in excluded_features:
                continue
            # Check for exact match first
            elif name in FEATURE_TRANSLATION:
                translated_index.append(FEATURE_TRANSLATION[name])
            # Check for caffeine variations
            elif 'CAFE√çNA' in name:
                translated_index.append(name.replace('CAFE√çNA', 'Caffeine'))
            else:
                translated_index.append(name)
        # Filter out excluded values too
        filtered_values = []
        original_index = []
        for i, name in enumerate(feature_series_or_list.index):
            excluded_features = ['ID PACIENTE', 'Unnamed: 0', 'Patient ID', 'Index',
                               'DELTA Repouso', 'DELTA Esfor√ßo', 'TEMPO TOTAL ATIVIDADE', 
                               'TEMPO PERMANENCIA', 'Rest Delta', 'Stress Delta', 
                               'Total Activity Time', 'Stay Duration']
            if name not in excluded_features:
                filtered_values.append(feature_series_or_list.values[i])
                original_index.append(name)
        
        return pd.Series(filtered_values, index=translated_index)
    else:
        # It's a list
        translated_list = []
        for name in feature_series_or_list:
            excluded_features = ['ID PACIENTE', 'Unnamed: 0', 'Patient ID', 'Index',
                               'DELTA Repouso', 'DELTA Esfor√ßo', 'TEMPO TOTAL ATIVIDADE', 
                               'TEMPO PERMANENCIA', 'Rest Delta', 'Stress Delta', 
                               'Total Activity Time', 'Stay Duration']
            if name in excluded_features:
                continue
            elif name in FEATURE_TRANSLATION:
                translated_list.append(FEATURE_TRANSLATION[name])
            elif 'CAFE√çNA' in name:
                translated_list.append(name.replace('CAFE√çNA', 'Caffeine'))
            else:
                translated_list.append(name)
        return translated_list

plt.style.use('default')
plt.rcParams['figure.facecolor'] = 'white'
plt.rcParams['axes.facecolor'] = 'white'
# Use non-interactive backend to avoid display issues
import matplotlib
matplotlib.use('Agg')

print("="*70)
print("MACHINE LEARNING ANALYSIS - SCINTIGRAPHY PROCEDURE REPETITION")
print("="*70)

# --- Leitura dos dados locais ---
try:
    df = pd.read_csv('dados_cintilografia.csv', encoding='utf-8')
    print(f"‚úÖ Dados carregados com sucesso. Shape: {df.shape}")
    print(f"Colunas dispon√≠veis: {list(df.columns)}")
except Exception as e:
    print(f"‚ùå Erro ao carregar os dados: {e}")
    exit()

# --- An√°lise inicial dos dados ---
print(f"\nüìä INFORMA√á√ïES B√ÅSICAS DO DATASET:")
print(f"N√∫mero de registros: {len(df)}")
print(f"N√∫mero de vari√°veis: {len(df.columns)}")

# Verificar valores √∫nicos na coluna target
if 'Repetiu' in df.columns:
    target_col = 'Repetiu'
elif 'REPETIU' in df.columns:
    target_col = 'REPETIU'
else:
    print("‚ùå Coluna de repeti√ß√£o n√£o encontrada!")
    print("Colunas dispon√≠veis:", df.columns.tolist())
    exit()

print(f"\nDistribui√ß√£o da vari√°vel alvo '{target_col}':")
print(df[target_col].value_counts())

# --- Pr√©-processamento ---
print("\nüîß INICIANDO PR√â-PROCESSAMENTO...")

# Criar c√≥pia para trabalhar
df_work = df.copy()

# Mapear coluna alvo para valores num√©ricos
if df_work[target_col].dtype == 'object':
    # Mapear diferentes formas de representar Sim/N√£o
    mapping = {'Sim': 1, 'SIM': 1, 'sim': 1, 'S': 1, 's': 1, 'True': 1, True: 1,
               'N√£o': 0, 'N√ÉO': 0, 'n√£o': 0, 'N': 0, 'n': 0, 'False': 0, False: 0}
    df_work[target_col] = df_work[target_col].fillna('N√£o').map(mapping)
    
    # Para valores que n√£o foram mapeados, assumir como "N√£o"
    df_work[target_col] = df_work[target_col].fillna(0)

print(f"Distribui√ß√£o ap√≥s mapeamento:")
print(df_work[target_col].value_counts())

# Identificar colunas num√©ricas e categ√≥ricas
numeric_cols = []
categorical_cols = []

for col in df_work.columns:
    if col == target_col:
        continue
    
    if df_work[col].dtype in ['int64', 'float64']:
        numeric_cols.append(col)
    else:
        # Tentar converter para num√©rico
        try:
            # Tratar v√≠rgulas decimais
            if df_work[col].dtype == 'object':
                df_work[col] = pd.to_numeric(df_work[col].astype(str).str.replace(',', '.'), errors='coerce')
                if not df_work[col].isna().all():
                    numeric_cols.append(col)
                else:
                    categorical_cols.append(col)
            else:
                categorical_cols.append(col)
        except:
            categorical_cols.append(col)

print(f"\nColunas num√©ricas identificadas ({len(numeric_cols)}): {numeric_cols}")
print(f"Colunas categ√≥ricas identificadas ({len(categorical_cols)}): {categorical_cols}")

# Processar colunas categ√≥ricas
if categorical_cols:
    # Preencher valores ausentes
    for col in categorical_cols:
        df_work[col] = df_work[col].fillna('N√ÉO INFORMADO')
    
    # Aplicar one-hot encoding sem prefix para evitar problemas
    df_categorical = pd.get_dummies(df_work[categorical_cols], drop_first=True)
    print(f"Vari√°veis categ√≥ricas ap√≥s encoding: {df_categorical.shape[1]}")
else:
    df_categorical = pd.DataFrame()

# Processar colunas num√©ricas
if numeric_cols:
    # Remove patient ID, index columns and specific timing variables as requested
    excluded_cols = ['ID PACIENTE', 'Unnamed: 0', 'DELTA Repouso', 'DELTA Esfor√ßo', 
                    'TEMPO TOTAL ATIVIDADE', 'TEMPO PERMANENCIA']
    clinical_numeric_cols = [col for col in numeric_cols if col not in excluded_cols]
    df_numeric = df_work[clinical_numeric_cols].fillna(df_work[clinical_numeric_cols].median())
    print(f"Vari√°veis num√©ricas processadas: {df_numeric.shape[1]} (excluindo IDs e vari√°veis de tempo)")
else:
    df_numeric = pd.DataFrame()

# Combinar features
if not df_categorical.empty and not df_numeric.empty:
    X = pd.concat([df_numeric, df_categorical], axis=1)
elif not df_numeric.empty:
    X = df_numeric
elif not df_categorical.empty:
    X = df_categorical
else:
    print("‚ùå Nenhuma feature v√°lida encontrada!")
    exit()

y = df_work[target_col].astype(int)

print(f"\nüìã DATASET FINAL:")
print(f"Features: {X.shape[1]} vari√°veis")
print(f"Samples: {X.shape[0]} registros")
print(f"Target distribution:")
print(y.value_counts())

# Verificar se h√° varia√ß√£o suficiente na target
if len(y.unique()) < 2:
    print("‚ùå N√£o h√° varia√ß√£o suficiente na vari√°vel target para criar um modelo!")
    exit()

# --- Divis√£o treino/teste ---
try:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.30, stratify=y, random_state=42
    )
    
    print(f"\n‚úÇÔ∏è DIVIS√ÉO DOS DADOS:")
    print(f"Treino: {len(X_train)} amostras")
    print(f"Teste: {len(X_test)} amostras")
    print(f"Distribui√ß√£o no treino: {y_train.value_counts().to_dict()}")
    print(f"Distribui√ß√£o no teste: {y_test.value_counts().to_dict()}")

except ValueError as e:
    print(f"‚ùå Erro na divis√£o dos dados: {e}")
    print("Tentando divis√£o sem estratifica√ß√£o...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.30, random_state=42
    )

# --- Balanceamento com SMOTE ---
print(f"\n‚öñÔ∏è APLICANDO BALANCEAMENTO SMOTE...")

try:
    smote = SMOTE(random_state=42)
    X_train_bal, y_train_bal = smote.fit_resample(X_train, y_train)
    
    print(f"Treino balanceado: {len(X_train_bal)} amostras")
    print(f"Distribui√ß√£o balanceada: {pd.Series(y_train_bal).value_counts().to_dict()}")
    
except Exception as e:
    print(f"‚ö†Ô∏è Erro no SMOTE: {e}")
    print("Utilizando dados originais sem balanceamento...")
    X_train_bal, y_train_bal = X_train, y_train

# --- Treinamento do Modelo ---
print(f"\nü§ñ TREINAMENTO DO MODELO RANDOM FOREST...")

model = RandomForestClassifier(
    n_estimators=100,
    random_state=42, 
    class_weight='balanced',
    max_depth=10,
    min_samples_split=5,
    min_samples_leaf=2
)

# Valida√ß√£o cruzada
try:
    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    cv_scores = cross_val_score(model, X_train_bal, y_train_bal, cv=skf, scoring='f1')
    print(f"F1-score m√©dio na valida√ß√£o cruzada: {cv_scores.mean():.3f} ¬± {cv_scores.std():.3f}")
except:
    print("Valida√ß√£o cruzada n√£o executada devido ao tamanho pequeno dos dados")

# Treinar modelo
model.fit(X_train_bal, y_train_bal)
print("‚úÖ Modelo treinado com sucesso!")

# --- Avalia√ß√£o ---
print(f"\nüìä AVALIA√á√ÉO DO MODELO:")

y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# M√©tricas b√°sicas
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, zero_division=0)
rec = recall_score(y_test, y_pred, zero_division=0)
f1 = f1_score(y_test, y_pred, zero_division=0)
auc = roc_auc_score(y_test, y_proba)

print(f"Acur√°cia: {acc:.3f}")
print(f"Precis√£o: {prec:.3f}")
print(f"Recall: {rec:.3f}")
print(f"F1-score: {f1:.3f}")
print(f"AUC-ROC: {auc:.3f}")

print(f"\nRelat√≥rio de Classifica√ß√£o:")
print(classification_report(y_test, y_pred))

print(f"\nMatriz de Confus√£o:")
cm = confusion_matrix(y_test, y_pred)
print(cm)

# --- Visualiza√ß√µes ---
print(f"\nüìà CRIANDO VISUALIZA√á√ïES...")

# 1. Import√¢ncia das Features
plt.figure(figsize=(12, 8))
if hasattr(model, 'feature_importances_'):
    importances = pd.Series(model.feature_importances_, index=X.columns)
    top_15 = importances.nlargest(15)
    
    # Translate feature names
    top_15_translated = translate_feature_names(top_15)
    
    ax = top_15_translated.plot(kind='barh', color=COLORS['secondary'], alpha=0.8)
    plt.title('Top 15 Most Important Features for Procedure Repetition Prediction', 
              fontsize=14, fontweight='bold', pad=20)
    plt.xlabel('Importance', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.gca().invert_yaxis()
    plt.grid(axis='x', alpha=0.3)
    plt.tight_layout()
    plt.savefig('feature_importance_repetition_en.png', dpi=300, bbox_inches='tight')
    plt.close()  # Close instead of show to avoid display issues

# 2. Curva ROC
plt.figure(figsize=(10, 8))
fpr, tpr, thresholds = roc_curve(y_test, y_proba)
plt.plot(fpr, tpr, linewidth=2, label=f'Random Forest (AUC = {auc:.3f})', 
         color=COLORS['primary'])
plt.plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.7, label='Baseline')
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve - Procedure Repetition Prediction', 
          fontsize=14, fontweight='bold', pad=20)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('roc_curve_repetition_en.png', dpi=300, bbox_inches='tight')
plt.close()

# 3. Matriz de Confus√£o Visualizada
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap='Blues', alpha=0.7)
plt.title('Confusion Matrix - Procedure Repetition Prediction', fontsize=14, fontweight='bold', pad=20)
plt.colorbar(label='Number of Cases')

# Adicionar texto nas c√©lulas
thresh = cm.max() / 2.
for i, j in np.ndindex(cm.shape):
    plt.text(j, i, format(cm[i, j], 'd'),
             horizontalalignment="center",
             color="white" if cm[i, j] > thresh else "black",
             fontsize=14, fontweight='bold')

plt.ylabel('True Value', fontsize=12)
plt.xlabel('Prediction', fontsize=12)
plt.xticks([0, 1], ['No Repetition', 'Repetition'])
plt.yticks([0, 1], ['No Repetition', 'Repetition'])
plt.tight_layout()
plt.savefig('confusion_matrix_repetition_en.png', dpi=300, bbox_inches='tight')
plt.close()

# 4. Distribui√ß√£o de Probabilidades
plt.figure(figsize=(12, 6))
prob_no_repeat = y_proba[y_test == 0]
prob_repeat = y_proba[y_test == 1]

plt.hist(prob_no_repeat, bins=20, alpha=0.7, label='No Repetition', 
         color=COLORS['quaternary'], density=True)
plt.hist(prob_repeat, bins=20, alpha=0.7, label='Repetition', 
         color=COLORS['primary'], density=True)

plt.xlabel('Predicted Probability of Repetition', fontsize=12)
plt.ylabel('Density', fontsize=12)
plt.title('Distribution of Predicted Probabilities', fontsize=14, fontweight='bold', pad=20)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('probability_distribution_repetition_en.png', dpi=300, bbox_inches='tight')
plt.close()

# 5. An√°lise de limiar otimizado
print(f"\nüéØ AN√ÅLISE DE LIMIAR OTIMIZADO:")

# Testar diferentes limiares
thresholds_test = np.arange(0.1, 1.0, 0.05)
f1_scores = []
precisions = []
recalls = []

for thresh in thresholds_test:
    y_pred_thresh = (y_proba >= thresh).astype(int)
    f1_scores.append(f1_score(y_test, y_pred_thresh, zero_division=0))
    precisions.append(precision_score(y_test, y_pred_thresh, zero_division=0))
    recalls.append(recall_score(y_test, y_pred_thresh, zero_division=0))

# Encontrar o melhor limiar
best_thresh_idx = np.argmax(f1_scores)
best_thresh = thresholds_test[best_thresh_idx]
best_f1 = f1_scores[best_thresh_idx]

print(f"Melhor limiar encontrado: {best_thresh:.2f}")
print(f"F1-score com melhor limiar: {best_f1:.3f}")

# Avalia√ß√£o com limiar otimizado
y_pred_optimized = (y_proba >= best_thresh).astype(int)

acc_opt = accuracy_score(y_test, y_pred_optimized)
prec_opt = precision_score(y_test, y_pred_optimized, zero_division=0)
rec_opt = recall_score(y_test, y_pred_optimized, zero_division=0)
f1_opt = f1_score(y_test, y_pred_optimized, zero_division=0)

print(f"\nM√©tricas com limiar otimizado ({best_thresh:.2f}):")
print(f"Acur√°cia: {acc_opt:.3f}")
print(f"Precis√£o: {prec_opt:.3f}")
print(f"Recall: {rec_opt:.3f}")
print(f"F1-score: {f1_opt:.3f}")

# 6. Gr√°fico de an√°lise de limiar
plt.figure(figsize=(12, 6))
plt.plot(thresholds_test, f1_scores, 'o-', label='F1-Score', color=COLORS['primary'], linewidth=2)
plt.plot(thresholds_test, precisions, 'o-', label='Precision', color=COLORS['secondary'], linewidth=2)
plt.plot(thresholds_test, recalls, 'o-', label='Recall', color=COLORS['tertiary'], linewidth=2)
plt.axvline(x=best_thresh, color=COLORS['quaternary'], linestyle='--', 
            label=f'Best Threshold ({best_thresh:.2f})')

plt.xlabel('Classification Threshold', fontsize=12)
plt.ylabel('Score', fontsize=12)
plt.title('Classification Threshold Analysis', fontsize=14, fontweight='bold', pad=20)
plt.legend(fontsize=11)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('threshold_analysis_repetition_en.png', dpi=300, bbox_inches='tight')
plt.close()

# --- Conclus√µes ---
print(f"\n" + "="*70)
print("RESUMO DA AN√ÅLISE")
print("="*70)

print(f"üìä DATASET:")
print(f"  ‚Ä¢ Total de registros: {len(df)}")
print(f"  ‚Ä¢ Features utilizadas: {X.shape[1]}")
print(f"  ‚Ä¢ Taxa de repeti√ß√£o: {y.mean():.1%}")

print(f"\nü§ñ PERFORMANCE DO MODELO:")
print(f"  ‚Ä¢ Acur√°cia: {acc:.1%}")
print(f"  ‚Ä¢ Precis√£o: {prec:.1%}")
print(f"  ‚Ä¢ Recall: {rec:.1%}")
print(f"  ‚Ä¢ F1-Score: {f1:.3f}")
print(f"  ‚Ä¢ AUC-ROC: {auc:.3f}")

print(f"\nüéØ LIMIAR OTIMIZADO:")
print(f"  ‚Ä¢ Melhor limiar: {best_thresh:.2f}")
print(f"  ‚Ä¢ F1-Score otimizado: {f1_opt:.3f}")
print(f"  ‚Ä¢ Recall otimizado: {rec_opt:.1%}")

print(f"\nüìÅ GENERATED FILES:")
print(f"  ‚Ä¢ feature_importance_repetition_en.png")
print(f"  ‚Ä¢ roc_curve_repetition_en.png")
print(f"  ‚Ä¢ confusion_matrix_repetition_en.png")
print(f"  ‚Ä¢ probability_distribution_repetition_en.png")
print(f"  ‚Ä¢ threshold_analysis_repetition_en.png")

print(f"\n‚úÖ AN√ÅLISE CONCLU√çDA COM SUCESSO!")

print(f"\nüîç TOP 5 MOST IMPORTANT FEATURES (English):")
if hasattr(model, 'feature_importances_'):
    importances = pd.Series(model.feature_importances_, index=X.columns)
    top_5 = importances.nlargest(5)
    top_5_translated = translate_feature_names(top_5)
    for i, (feature, importance) in enumerate(top_5_translated.items(), 1):
        print(f"  {i}. {feature}: {importance:.3f}")
        
print(f"\nüìä ENGLISH FEATURE NAMES MAPPING:")
excluded_features = ['ID PACIENTE', 'Unnamed: 0', 'DELTA Repouso', 'DELTA Esfor√ßo', 
                    'TEMPO TOTAL ATIVIDADE', 'TEMPO PERMANENCIA']
used_features = [col for col in X.columns.tolist()[:10] if col not in excluded_features]  # Show first 10 clinical features
print("  Original ‚Üí English:")
for feature in used_features:
    translated = FEATURE_TRANSLATION.get(feature, feature)
    if translated != feature:
        print(f"  ‚Ä¢ {feature} ‚Üí {translated}")
    elif 'CAFE√çNA' in feature:
        print(f"  ‚Ä¢ {feature} ‚Üí {feature.replace('CAFE√çNA', 'Caffeine')}")
    else:
        print(f"  ‚Ä¢ {feature} (unchanged)")